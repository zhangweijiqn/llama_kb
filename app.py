import os
import gradio as gr
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.document_loaders import PyPDFLoader, TextLoader
from langchain_community.llms import LlamaCpp
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain.chains import RetrievalQA

# ==================== é…ç½®åŒºåŸŸ ====================
# æ¨¡å‹è·¯å¾„ï¼Œè¯·ä¿®æ”¹ä¸ºä½ ä¸‹è½½çš„æ¨¡å‹æ–‡ä»¶å®é™…è·¯å¾„
MODEL_PATH = "./llama-3.2-3b-instruct-q4_0.gguf"
# çŸ¥è¯†åº“æ–‡æ¡£å­˜æ”¾ç›®å½•
DOCS_DIR = "./docs"
# å‘é‡æ•°æ®åº“æŒä¹…åŒ–ç›®å½•
PERSIST_DIR = "./chroma_db"
# ==================== é…ç½®ç»“æŸ ====================

from langchain.prompts import PromptTemplate

# è‡ªå®šä¹‰æç¤ºè¯
prompt_template = """åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œè¯·ç›´æ¥å›ç­”é—®é¢˜ã€‚å¦‚æœæ— æ³•ä»ä¸Šä¸‹æ–‡ä¸­å¾—åˆ°ç­”æ¡ˆï¼Œè¯·è¯´ä¸çŸ¥é“ã€‚

ä¸Šä¸‹æ–‡ï¼š
{context}

é—®é¢˜ï¼š
{question}

ç­”æ¡ˆï¼š"""

PROMPT = PromptTemplate(
    template=prompt_template, input_variables=["context", "question"]
)


# åˆå§‹åŒ–æ–‡æœ¬åµŒå…¥æ¨¡å‹ï¼ˆç”¨äºå°†æ–‡æœ¬è½¬æ¢ä¸ºå‘é‡ï¼‰
embeddings = HuggingFaceEmbeddings(
    model_name="all-MiniLM-L6-v2"  # ä¸€ä¸ªè½»é‡ä¸”é«˜æ•ˆçš„å¥å­åµŒå…¥æ¨¡å‹
)

# åˆå§‹åŒ–LLaMAæ¨¡å‹
def load_llm():
    llm = LlamaCpp(
        model_path=MODEL_PATH,
        n_ctx=4096,           # ä¸Šä¸‹æ–‡é•¿åº¦ï¼Œå…è®¸æ›´é•¿çš„æ–‡æ¡£å¤„ç†
        n_batch=512,          # æ‰¹å¤„ç†å¤§å°ï¼Œæé«˜å¤„ç†æ•ˆç‡
        n_gpu_layers=0,       # ä½¿ç”¨GPUçš„å±‚æ•°ï¼Œ0è¡¨ç¤ºä»…ç”¨CPUã€‚å¦‚æœ‰GPUï¼Œå¯è®¾ç½®ä¸º35-50åŠ é€Ÿ
        verbose=False,        # æ˜¯å¦æ‰“å°è¯¦ç»†æ—¥å¿—
        temperature=0.2,      # æ§åˆ¶ç”Ÿæˆéšæœºæ€§ï¼ˆ0-1ï¼‰ï¼Œå€¼è¶Šä½ç­”æ¡ˆè¶Šç¡®å®š
        max_tokens=512,       # ç”Ÿæˆå›ç­”çš„æœ€å¤§é•¿åº¦
    )
    return llm

# åˆå§‹åŒ–æˆ–åŠ è½½å‘é‡æ•°æ®åº“
def init_vectorstore():
    if os.path.exists(PERSIST_DIR):
        # å¦‚æœå·²å­˜åœ¨ï¼Œåˆ™ç›´æ¥åŠ è½½
        print("åŠ è½½å·²æœ‰çš„å‘é‡æ•°æ®åº“...")
        return Chroma(persist_directory=PERSIST_DIR, embedding_function=embeddings)
    else:
        # å¦åˆ™ï¼Œåˆ›å»ºä¸€ä¸ªç©ºçš„Chromaæ•°æ®åº“
        print("åˆ›å»ºæ–°çš„å‘é‡æ•°æ®åº“...")
        texts = ["è¿™æ˜¯ä½ çŸ¥è¯†åº“çš„åˆå§‹æ–‡æ¡£ã€‚è¯·é€šè¿‡ç•Œé¢æ·»åŠ ä½ è‡ªå·±çš„æ–‡æ¡£ã€‚"]
        return Chroma.from_texts(texts, embeddings, persist_directory=PERSIST_DIR)

# å¤„ç†ä¸Šä¼ çš„æ–‡æ¡£ï¼Œæ„å»ºçŸ¥è¯†åº“
def process_documents():
    doc_files = []
    for filename in os.listdir(DOCS_DIR):
        file_path = os.path.join(DOCS_DIR, filename)
        if filename.endswith('.pdf'):
            loader = PyPDFLoader(file_path)
        elif filename.endswith('.txt'):
            loader = TextLoader(file_path, encoding='utf-8')
        else:
            continue  # è·³è¿‡ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹
        doc_files.extend(loader.load())
    
    if not doc_files:
        return "æœªåœ¨ 'docs' ç›®å½•ä¸‹æ‰¾åˆ°æ”¯æŒçš„æ–‡æ¡£ï¼ˆ.pdf æˆ– .txtï¼‰ã€‚"
    
    # å°†æ–‡æ¡£åˆ‡åˆ†æˆå—
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=500,   # æ¯ä¸ªæ–‡æœ¬å—çš„å¤§å°
        chunk_overlap=50   # å—ä¹‹é—´çš„é‡å éƒ¨åˆ†ï¼Œé¿å…è¯­ä¹‰æ–­è£‚
    )
    chunks = text_splitter.split_documents(doc_files)
    
    # å°†æ–‡æœ¬å—æ·»åŠ åˆ°å‘é‡æ•°æ®åº“
    global vector_db
    vector_db = Chroma.from_documents(chunks, embeddings, persist_directory=PERSIST_DIR)
    return f"çŸ¥è¯†åº“æ„å»ºæˆåŠŸï¼å¤„ç†äº† {len(doc_files)} ä¸ªæ–‡æ¡£ï¼Œå…±ç”Ÿæˆ {len(chunks)} ä¸ªæ–‡æœ¬å—ã€‚"

# é—®ç­”å‡½æ•°
def ask_question(question, history):
    if 'vector_db' not in globals():
        return "è¯·å…ˆåˆå§‹åŒ–æˆ–æ„å»ºçŸ¥è¯†åº“ã€‚", history
    
    # ä»å‘é‡æ•°æ®åº“ä¸­æ£€ç´¢ä¸é—®é¢˜æœ€ç›¸å…³çš„æ–‡æ¡£ç‰‡æ®µ
    retriever = vector_db.as_retriever(search_kwargs={"k": 3})  # è¿”å›æœ€ç›¸å…³çš„3ä¸ªç‰‡æ®µ
    
    # åˆ›å»ºæ£€ç´¢é—®ç­”é“¾
    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",  # å°†æ£€ç´¢åˆ°çš„å†…å®¹"å¡«å……"åˆ°æç¤ºè¯ä¸­
        retriever=retriever,
        return_source_documents=True,   # æ˜¯å¦è¿”å›å¼•ç”¨æºæ–‡æ¡£
        chain_type_kwargs={"prompt": PROMPT}  # ä½¿ç”¨è‡ªå®šä¹‰æç¤ºè¯
    )
    
    # æ‰§è¡Œé—®ç­”
    result = qa_chain.invoke({"query": question})
    print('result:', result)
    history.append([question, result["result"]])
    return "", history  # æ¸…ç©ºè¾“å…¥æ¡†ï¼Œæ›´æ–°å†å²

# åˆå§‹åŒ–æ ¸å¿ƒç»„ä»¶
print("æ­£åœ¨åˆå§‹åŒ–æ¨¡å‹å’Œå‘é‡æ•°æ®åº“...")
llm = load_llm()
vector_db = init_vectorstore()

# æ„å»ºGradioç•Œé¢
with gr.Blocks(title="LLaMAä¸ªäººçŸ¥è¯†åº“é—®ç­”ç³»ç»Ÿ", theme=gr.themes.Soft()) as demo:
    gr.Markdown("# ğŸ¦™ LLaMA ä¸ªäººçŸ¥è¯†åº“é—®ç­”ç³»ç»Ÿ")
    gr.Markdown("ä¸Šä¼ ä½ çš„æ–‡æ¡£åˆ°`docs`ç›®å½•ï¼Œç„¶åç‚¹å‡»**æ„å»ºçŸ¥è¯†åº“**ã€‚å®Œæˆåå°±å¯ä»¥å¼€å§‹æé—®äº†ï¼")
    
    with gr.Row():
        with gr.Column(scale=1):
            gr.Markdown("## çŸ¥è¯†åº“ç®¡ç†")
            build_btn = gr.Button("ğŸš€ æ„å»º/æ›´æ–°çŸ¥è¯†åº“", variant="primary")
            build_output = gr.Textbox(label="æ„å»ºçŠ¶æ€", interactive=False)
        
        with gr.Column(scale=2):
            gr.Markdown("## é—®ç­”ç•Œé¢")
            chatbot = gr.Chatbot(label="å¯¹è¯å†å²", height=400)
            question_input = gr.Textbox(
                label="è¯·è¾“å…¥ä½ çš„é—®é¢˜",
                placeholder="ä¾‹å¦‚ï¼šæ–‡æ¡£ä¸­æåˆ°çš„XXæ˜¯ä»€ä¹ˆï¼Ÿ",
                lines=2
            )
            submit_btn = gr.Button("å‘é€", variant="primary")
            clear_btn = gr.Button("æ¸…ç©ºå¯¹è¯")
    
    # ç»‘å®šæŒ‰é’®äº‹ä»¶
    build_btn.click(fn=process_documents, outputs=build_output)
    submit_btn.click(fn=ask_question, inputs=[question_input, chatbot], outputs=[question_input, chatbot])
    question_input.submit(fn=ask_question, inputs=[question_input, chatbot], outputs=[question_input, chatbot])
    clear_btn.click(lambda: None, None, chatbot, queue=False)

# å¯åŠ¨åº”ç”¨
if __name__ == "__main__":
    demo.launch(server_name="0.0.0.0", server_port=7860, share=False)
